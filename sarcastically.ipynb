{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95acebc0",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from common.metrics import f1_m, recall_m, precision_m\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c31e86",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   file_name                                          utterance  \\\n0   1_60.wav  It's just a privilege to watch your mind at work.   \n1   1_70.wav  I don't think I'll be able to stop thinking ab...   \n2   1_80.wav  Since it's not bee season, you can have my epi...   \n3   1_90.wav  Lois Lane is falling, accelerating at an initi...   \n4  1_105.wav  I'm just inferring this is a couch because the...   \n\n                                             context  sarcasm  \n0  ['I never would have identified the fingerprin...     True  \n1  ['This is one of my favorite places to kick ba...     True  \n2  ['Here we go. Pad thai, no peanuts.', 'But doe...    False  \n3  ['A marathon? How many Superman movies are the...    False  \n4  [\"Great Caesar's ghost, look at this place.\", ...     True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>utterance</th>\n      <th>context</th>\n      <th>sarcasm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1_60.wav</td>\n      <td>It's just a privilege to watch your mind at work.</td>\n      <td>['I never would have identified the fingerprin...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1_70.wav</td>\n      <td>I don't think I'll be able to stop thinking ab...</td>\n      <td>['This is one of my favorite places to kick ba...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1_80.wav</td>\n      <td>Since it's not bee season, you can have my epi...</td>\n      <td>['Here we go. Pad thai, no peanuts.', 'But doe...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1_90.wav</td>\n      <td>Lois Lane is falling, accelerating at an initi...</td>\n      <td>['A marathon? How many Superman movies are the...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1_105.wav</td>\n      <td>I'm just inferring this is a couch because the...</td>\n      <td>[\"Great Caesar's ghost, look at this place.\", ...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('normalized_mustard_dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd25d582",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embedding_size = 16\n",
    "max_length = 60\n",
    "padding_type = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d3a752b3",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   file_name                                          utterance  \\\n0   1_60.wav  It's just a privilege to watch your mind at work.   \n1   1_70.wav  I don't think I'll be able to stop thinking ab...   \n2   1_80.wav  Since it's not bee season, you can have my epi...   \n3   1_90.wav  Lois Lane is falling, accelerating at an initi...   \n4  1_105.wav  I'm just inferring this is a couch because the...   \n\n                                             context  sarcasm  \n0  ['I never would have identified the fingerprin...        1  \n1  ['This is one of my favorite places to kick ba...        1  \n2  ['Here we go. Pad thai, no peanuts.', 'But doe...        0  \n3  ['A marathon? How many Superman movies are the...        0  \n4  [\"Great Caesar's ghost, look at this place.\", ...        1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>utterance</th>\n      <th>context</th>\n      <th>sarcasm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1_60.wav</td>\n      <td>It's just a privilege to watch your mind at work.</td>\n      <td>['I never would have identified the fingerprin...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1_70.wav</td>\n      <td>I don't think I'll be able to stop thinking ab...</td>\n      <td>['This is one of my favorite places to kick ba...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1_80.wav</td>\n      <td>Since it's not bee season, you can have my epi...</td>\n      <td>['Here we go. Pad thai, no peanuts.', 'But doe...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1_90.wav</td>\n      <td>Lois Lane is falling, accelerating at an initi...</td>\n      <td>['A marathon? How many Superman movies are the...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1_105.wav</td>\n      <td>I'm just inferring this is a couch because the...</td>\n      <td>[\"Great Caesar's ghost, look at this place.\", ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sarcasm'].replace({True:1,False:0},inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd7bac07",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "utterances = data['utterance'].values\n",
    "sarcasm_states = data['sarcasm'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49fae700",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(utterances)\n",
    "train = tokenizer.texts_to_sequences(utterances)\n",
    "padded_train_sequences = pad_sequences(train, maxlen=max_length, padding=padding_type)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_text_train = to_categorical(label_encoder.fit_transform(sarcasm_states))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d08d05bd",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     file_name                                          utterance  \\\n685  2_169.wav  Hes not right for the part, and if I suggest h...   \n686  2_235.wav  Oh yeah he has a caretaker his older brother, ...   \n687   2_34.wav  Is it me or the greetings gone downhill around...   \n688  2_608.wav  You are right, by saying nice, I am virtually ...   \n689  2_524.wav            Yes and we are \"very\" excited about it.   \n\n                                               context  sarcasm  \n685  ['What am I gonna do now?', 'Just pass the tap...        1  \n686  ['Helo! Anybody in there order a celebrity?', ...        0  \n687                     ['Hey', 'You son of a bitch!']        1  \n688  ['Did I go to this school?', \"Hey, there's Mis...        1  \n689  [\"Anyway, if you don't feel like being alone t...        1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>utterance</th>\n      <th>context</th>\n      <th>sarcasm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>685</th>\n      <td>2_169.wav</td>\n      <td>Hes not right for the part, and if I suggest h...</td>\n      <td>['What am I gonna do now?', 'Just pass the tap...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>686</th>\n      <td>2_235.wav</td>\n      <td>Oh yeah he has a caretaker his older brother, ...</td>\n      <td>['Helo! Anybody in there order a celebrity?', ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>687</th>\n      <td>2_34.wav</td>\n      <td>Is it me or the greetings gone downhill around...</td>\n      <td>['Hey', 'You son of a bitch!']</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>688</th>\n      <td>2_608.wav</td>\n      <td>You are right, by saying nice, I am virtually ...</td>\n      <td>['Did I go to this school?', \"Hey, there's Mis...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>689</th>\n      <td>2_524.wav</td>\n      <td>Yes and we are \"very\" excited about it.</td>\n      <td>[\"Anyway, if you don't feel like being alone t...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8e82d196",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(padded_train_sequences, y_text_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f311c624",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2be61ca8",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_fasttext_vectors2(fname):\n",
    "    embeddings_index = {}\n",
    "    f = codecs.open(fname, encoding='utf-8')\n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        ft_word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[ft_word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b3e1fcb",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999995it [01:22, 12147.42it/s]\n"
     ]
    }
   ],
   "source": [
    "w2v_model =  load_fasttext_vectors2(\"wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5b07d323",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, Bidirectional, Conv1D, MaxPooling1D, Input, Embedding\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ec9dc659",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "words_not_found = []\n",
    "nb_words = len(tokenizer.word_index)\n",
    "embedding_matrix = np.zeros((nb_words + 1, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = w2v_model.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e4028782",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      [(None, 60)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 60, 300)           610800    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 57, 50)            60050     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 28, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 28, 128)           58880     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3584)              0         \n",
      "_________________________________________________________________\n",
      "fully_connected (Dense)      (None, 75)                268875    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 152       \n",
      "=================================================================\n",
      "Total params: 998,757\n",
      "Trainable params: 387,957\n",
      "Non-trainable params: 610,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_data = Input(shape=(max_length,), name='main_input')\n",
    "embedding_layer = Embedding(vocab_size + 1, 300, weights=[embedding_matrix], trainable=False)(input_data)\n",
    "conv_1 = Conv1D(filters=50, kernel_size=4, activation='relu')(embedding_layer)\n",
    "max_1 = MaxPooling1D(pool_size=2)(conv_1)\n",
    "lstm_layer = Bidirectional(LSTM(64,return_sequences=True))(max_1)\n",
    "\n",
    "flatten = Flatten()(lstm_layer)\n",
    "dense = Dense(75, activation='relu', name='fully_connected')(flatten)\n",
    "out = Dense(2, activation='softmax')(dense)\n",
    "\n",
    "model = Model(inputs=[input_data], outputs=[out])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "abc13eef",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy', f1_m, precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b1bd7fe0",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 552 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 02:26:02.482000: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_49342_49526' and '__inference___backward_standard_lstm_50205_50692_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_50969' both implement 'lstm_c87532b8-8a82-4166-8a55-939d8f29c262' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 6s 11ms/sample - loss: 0.7030 - accuracy: 0.5109 - f1_m: 0.5115 - precision_m: 0.5115 - recall_m: 0.5115\n",
      "Epoch 2/5\n",
      "552/552 [==============================] - 1s 2ms/sample - loss: 0.6888 - accuracy: 0.5417 - f1_m: 0.5389 - precision_m: 0.5389 - recall_m: 0.5389\n",
      "Epoch 3/5\n",
      "552/552 [==============================] - 1s 2ms/sample - loss: 0.6767 - accuracy: 0.5707 - f1_m: 0.5698 - precision_m: 0.5698 - recall_m: 0.5698\n",
      "Epoch 4/5\n",
      "552/552 [==============================] - 1s 2ms/sample - loss: 0.6453 - accuracy: 0.6250 - f1_m: 0.6240 - precision_m: 0.6240 - recall_m: 0.6240\n",
      "Epoch 5/5\n",
      "552/552 [==============================] - 1s 2ms/sample - loss: 0.5073 - accuracy: 0.7681 - f1_m: 0.7694 - precision_m: 0.7694 - recall_m: 0.7694\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_text_train, y_text_train, batch_size=64, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing Text Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 02:26:13.188909: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_51548' and '__inference_standard_lstm_51548_specialized_for_model_2_bidirectional_2_forward_lstm_2_StatefulPartitionedCall_at___inference_distributed_function_52524' both implement 'lstm_1e88a22b-f97a-4950-a32f-7594141cb787' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7793718602346338\n",
      "accuracy:0.6521739363670349\n",
      "f1_score:0.6862499117851257\n",
      "precision:0.6862500309944153\n",
      "recall:0.6862500309944153\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X_text_test, y_text_test, verbose=0)\n",
    "print(f\"loss: {loss}\\naccuracy:{accuracy}\\nf1_score:{f1_score}\\nprecision:{precision}\\nrecall:{recall}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d31ba5bd",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 02:26:17.741308: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_standard_lstm_52818_specialized_for_model_2_bidirectional_2_forward_lstm_2_StatefulPartitionedCall_at___inference_distributed_function_53609' and '__inference_cudnn_lstm_with_fallback_52929' both implement 'lstm_3e93ff0c-1bc2-47ab-b822-8d0d3fe597a7' but their signatures do not match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-sarcastic\n",
      "[0.7925658  0.20743422]\n"
     ]
    }
   ],
   "source": [
    "user_statement = input(\"Tell me something sarcastic: \\n\")\n",
    "tokenized_statement = tokenizer.texts_to_sequences([user_statement])\n",
    "tokenized_statement = pad_sequences(tokenized_statement, maxlen=max_length, padding=padding_type)\n",
    "output = model.predict(tokenized_statement)[0]\n",
    "\n",
    "if np.argmax(output) == 0:\n",
    "    print(\"Non-sarcastic\")\n",
    "elif np.argmax(output) == 1:\n",
    "    print(\"Sarcasm\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "47fc9839",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"models/text_model.h5\", overwrite=True, save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7598d04",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Audio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a54b58",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "40413eef",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "audio_data_fp = \"mmsd_raw_data/converted_utterances\"\n",
    "dataset_csv_path = \"normalized_mustard_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a460138e",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_mfcc(filepath:str, n_fft:int, hop_length: int, n_mfcc:int = 13):\n",
    "    \"\"\"\n",
    "    Creates MFCC for file at filepath\n",
    "\n",
    "    :param filepath: Location of file to be used\n",
    "    :param n_fft: Number of Fast Fourier Transforms\n",
    "    :param hop_length: Number of Hops within samples\n",
    "    :param n_mfcc: Number of MFCC's to be outputted\n",
    "    :return: Array containing mean of all MFCC's\n",
    "    \"\"\"\n",
    "    signal, sample_rate = librosa.load(filepath)\n",
    "    mfccs = librosa.feature.mfcc(y=signal, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=n_mfcc)\n",
    "    mean_mfccs = np.mean(mfccs.T,axis=0)\n",
    "    return mean_mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72a55718",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_mfcc_features(data):\n",
    "    \"\"\"\n",
    "    Creates a list of Mel-Frequency Co-Efficients\n",
    "    :param data: Pandas Dataframe of Input Data\n",
    "    \"\"\"\n",
    "    hop_length = 512\n",
    "    n_fft = 2048\n",
    "\n",
    "    _mfcc_df = pd.DataFrame(columns=[\"features\", \"sarcasm_state\"])\n",
    "\n",
    "    tqdm_data = tqdm(zip(data[\"file_name\"], data[\"sarcasm\"]))\n",
    "\n",
    "    for file_name,sarcasm_state in tqdm_data:\n",
    "        tqdm_data.set_description(f\"Creating MFCC for {file_name}\")\n",
    "        fp = f\"{audio_data_fp}/{str(file_name)}\"\n",
    "        mean_mfccs = create_mfcc(fp, n_fft, hop_length)\n",
    "        _mfcc_df = _mfcc_df.append({\n",
    "            \"features\": mean_mfccs,\n",
    "            \"sarcasm_state\": sarcasm_state\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    return _mfcc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2ad3292f",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating MFCC for 2_524.wav: : 690it [02:44,  4.21it/s]  \n"
     ]
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(dataset_csv_path)\n",
    "mfcc_df = create_mfcc_features(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "31729dc6",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                            features sarcasm_state\n0  [-146.32887, 99.83219, -50.631638, 9.274939, -...          True\n1  [-147.90367, 104.595116, -39.535, 4.108271, -2...          True\n2  [-21.120022, 86.11223, -35.381474, 22.640253, ...         False\n3  [-23.78374, 70.78856, -34.25742, 23.38274, -16...         False\n4  [1.6350226, 82.73289, -46.96253, 14.140306, -2...          True",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>sarcasm_state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-146.32887, 99.83219, -50.631638, 9.274939, -...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-147.90367, 104.595116, -39.535, 4.108271, -2...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[-21.120022, 86.11223, -35.381474, 22.640253, ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[-23.78374, 70.78856, -34.25742, 23.38274, -16...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[1.6350226, 82.73289, -46.96253, 14.140306, -2...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4cac8d5e",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(mfcc_df['features'].tolist())\n",
    "y = np.array(mfcc_df['sarcasm_state'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "acb36ac2",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(690, 13)"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "364bbd2d",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Label Encoder for getting sarcasm state\n",
    "label_encoder = LabelEncoder()\n",
    "y = to_categorical(label_encoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c49dd06f",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(690, 2)"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c9386d5e",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_audio_train, X_audio_test, y_audio_train, y_audio_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bcc06f04",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Number of Classes\n",
    "label_count = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "55034f53",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 100)               1400      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 41,902\n",
      "Trainable params: 41,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(13,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(label_count))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "52c76993",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", metrics=['accuracy',f1_m, precision_m, recall_m], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6d7cb405",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 552 samples\n",
      "Epoch 1/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 22.1317 - accuracy: 0.4969 - f1_m: 0.4969 - precision_m: 0.4969 - recall_m: 0.4969 WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 1s 2ms/sample - loss: 19.8755 - accuracy: 0.5163 - f1_m: 0.5208 - precision_m: 0.5208 - recall_m: 0.5208\n",
      "Epoch 2/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 12.9518 - accuracy: 0.5156 - f1_m: 0.5156 - precision_m: 0.5156 - recall_m: 0.5156WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 189us/sample - loss: 12.2448 - accuracy: 0.5181 - f1_m: 0.5174 - precision_m: 0.5174 - recall_m: 0.5174\n",
      "Epoch 3/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 10.1526 - accuracy: 0.5125 - f1_m: 0.5125 - precision_m: 0.5125 - recall_m: 0.5125WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 199us/sample - loss: 9.7528 - accuracy: 0.5181 - f1_m: 0.5174 - precision_m: 0.5174 - recall_m: 0.5174\n",
      "Epoch 4/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 6.9520 - accuracy: 0.5281 - f1_m: 0.5281 - precision_m: 0.5281 - recall_m: 0.5281WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 199us/sample - loss: 6.4151 - accuracy: 0.5380 - f1_m: 0.5365 - precision_m: 0.5365 - recall_m: 0.5365\n",
      "Epoch 5/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 6.1001 - accuracy: 0.5511 - f1_m: 0.5511 - precision_m: 0.5511 - recall_m: 0.5511WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 192us/sample - loss: 5.6822 - accuracy: 0.5580 - f1_m: 0.5608 - precision_m: 0.5608 - recall_m: 0.5608\n",
      "Epoch 6/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 4.4296 - accuracy: 0.5426 - f1_m: 0.5426 - precision_m: 0.5426 - recall_m: 0.5426WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 177us/sample - loss: 4.4887 - accuracy: 0.5507 - f1_m: 0.5538 - precision_m: 0.5538 - recall_m: 0.5538\n",
      "Epoch 7/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 4.0470 - accuracy: 0.5312 - f1_m: 0.5312 - precision_m: 0.5312 - recall_m: 0.5312WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 179us/sample - loss: 3.9444 - accuracy: 0.5254 - f1_m: 0.5243 - precision_m: 0.5243 - recall_m: 0.5243\n",
      "Epoch 8/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 3.9759 - accuracy: 0.5227 - f1_m: 0.5227 - precision_m: 0.5227 - recall_m: 0.5227WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 184us/sample - loss: 3.7350 - accuracy: 0.5217 - f1_m: 0.5208 - precision_m: 0.5208 - recall_m: 0.5208\n",
      "Epoch 9/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 3.0013 - accuracy: 0.5682 - f1_m: 0.5682 - precision_m: 0.5682 - recall_m: 0.5682WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 177us/sample - loss: 2.9396 - accuracy: 0.5525 - f1_m: 0.5608 - precision_m: 0.5608 - recall_m: 0.5608\n",
      "Epoch 10/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 2.5899 - accuracy: 0.5594 - f1_m: 0.5594 - precision_m: 0.5594 - recall_m: 0.5594WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 193us/sample - loss: 2.3336 - accuracy: 0.5688 - f1_m: 0.5608 - precision_m: 0.5608 - recall_m: 0.5608\n",
      "Epoch 11/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 2.2423 - accuracy: 0.5341 - f1_m: 0.5341 - precision_m: 0.5341 - recall_m: 0.5341WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 184us/sample - loss: 2.1427 - accuracy: 0.5362 - f1_m: 0.5347 - precision_m: 0.5347 - recall_m: 0.5347\n",
      "Epoch 12/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 1.8298 - accuracy: 0.5653 - f1_m: 0.5653 - precision_m: 0.5653 - recall_m: 0.5653WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 179us/sample - loss: 1.8278 - accuracy: 0.5707 - f1_m: 0.5625 - precision_m: 0.5625 - recall_m: 0.5625\n",
      "Epoch 13/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 1.5503 - accuracy: 0.5795 - f1_m: 0.5795 - precision_m: 0.5795 - recall_m: 0.5795WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 177us/sample - loss: 1.5650 - accuracy: 0.5688 - f1_m: 0.5816 - precision_m: 0.5816 - recall_m: 0.5816\n",
      "Epoch 14/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 1.5808 - accuracy: 0.4886 - f1_m: 0.4886 - precision_m: 0.4886 - recall_m: 0.4886WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 175us/sample - loss: 1.4902 - accuracy: 0.5290 - f1_m: 0.5434 - precision_m: 0.5434 - recall_m: 0.5434\n",
      "Epoch 15/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 1.4867 - accuracy: 0.5398 - f1_m: 0.5398 - precision_m: 0.5398 - recall_m: 0.5398WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 184us/sample - loss: 1.4050 - accuracy: 0.5580 - f1_m: 0.5660 - precision_m: 0.5660 - recall_m: 0.5660\n",
      "Epoch 16/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 1.2918 - accuracy: 0.5483 - f1_m: 0.5483 - precision_m: 0.5483 - recall_m: 0.5483WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 180us/sample - loss: 1.2750 - accuracy: 0.5453 - f1_m: 0.5434 - precision_m: 0.5434 - recall_m: 0.5434\n",
      "Epoch 17/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 1.2597 - accuracy: 0.5284 - f1_m: 0.5284 - precision_m: 0.5284 - recall_m: 0.5284WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 182us/sample - loss: 1.1681 - accuracy: 0.5308 - f1_m: 0.5191 - precision_m: 0.5191 - recall_m: 0.5191\n",
      "Epoch 18/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 1.0954 - accuracy: 0.5511 - f1_m: 0.5511 - precision_m: 0.5511 - recall_m: 0.5511WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 178us/sample - loss: 1.0038 - accuracy: 0.5670 - f1_m: 0.5538 - precision_m: 0.5538 - recall_m: 0.5538\n",
      "Epoch 19/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.9792 - accuracy: 0.5597 - f1_m: 0.5597 - precision_m: 0.5597 - recall_m: 0.5597WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 178us/sample - loss: 0.9813 - accuracy: 0.5507 - f1_m: 0.5538 - precision_m: 0.5538 - recall_m: 0.5538\n",
      "Epoch 20/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.9750 - accuracy: 0.5844 - f1_m: 0.5844 - precision_m: 0.5844 - recall_m: 0.5844WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 205us/sample - loss: 0.9893 - accuracy: 0.5507 - f1_m: 0.5434 - precision_m: 0.5434 - recall_m: 0.5434\n",
      "Epoch 21/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.9672 - accuracy: 0.5813 - f1_m: 0.5812 - precision_m: 0.5813 - recall_m: 0.5813WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 193us/sample - loss: 0.9403 - accuracy: 0.5779 - f1_m: 0.5747 - precision_m: 0.5747 - recall_m: 0.5747\n",
      "Epoch 22/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.9256 - accuracy: 0.5767 - f1_m: 0.5767 - precision_m: 0.5767 - recall_m: 0.5767WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 190us/sample - loss: 0.9200 - accuracy: 0.5797 - f1_m: 0.5920 - precision_m: 0.5920 - recall_m: 0.5920\n",
      "Epoch 23/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.9111 - accuracy: 0.5483 - f1_m: 0.5483 - precision_m: 0.5483 - recall_m: 0.5483WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 189us/sample - loss: 0.9214 - accuracy: 0.5580 - f1_m: 0.5556 - precision_m: 0.5556 - recall_m: 0.5556\n",
      "Epoch 24/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.7614 - accuracy: 0.5767 - f1_m: 0.5767 - precision_m: 0.5767 - recall_m: 0.5767WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 188us/sample - loss: 0.7926 - accuracy: 0.5725 - f1_m: 0.5747 - precision_m: 0.5747 - recall_m: 0.5747\n",
      "Epoch 25/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.9195 - accuracy: 0.5199 - f1_m: 0.5199 - precision_m: 0.5199 - recall_m: 0.5199WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 189us/sample - loss: 0.8794 - accuracy: 0.5362 - f1_m: 0.5451 - precision_m: 0.5451 - recall_m: 0.5451\n",
      "Epoch 26/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.8116 - accuracy: 0.5909 - f1_m: 0.5909 - precision_m: 0.5909 - recall_m: 0.5909WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 187us/sample - loss: 0.7946 - accuracy: 0.5996 - f1_m: 0.6007 - precision_m: 0.6007 - recall_m: 0.6007\n",
      "Epoch 27/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.7966 - accuracy: 0.5795 - f1_m: 0.5795 - precision_m: 0.5795 - recall_m: 0.5795WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 188us/sample - loss: 0.7970 - accuracy: 0.5833 - f1_m: 0.5851 - precision_m: 0.5851 - recall_m: 0.5851\n",
      "Epoch 28/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.7273 - accuracy: 0.6080 - f1_m: 0.6080 - precision_m: 0.6080 - recall_m: 0.6080WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 189us/sample - loss: 0.7195 - accuracy: 0.6286 - f1_m: 0.6285 - precision_m: 0.6285 - recall_m: 0.6285\n",
      "Epoch 29/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.7833 - accuracy: 0.6062 - f1_m: 0.6062 - precision_m: 0.6062 - recall_m: 0.6062WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 198us/sample - loss: 0.7899 - accuracy: 0.5924 - f1_m: 0.5937 - precision_m: 0.5938 - recall_m: 0.5938\n",
      "Epoch 30/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.7908 - accuracy: 0.6222 - f1_m: 0.6222 - precision_m: 0.6222 - recall_m: 0.6222WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 187us/sample - loss: 0.7749 - accuracy: 0.6033 - f1_m: 0.5990 - precision_m: 0.5990 - recall_m: 0.5990\n",
      "Epoch 31/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.7539 - accuracy: 0.5938 - f1_m: 0.5937 - precision_m: 0.5938 - recall_m: 0.5938WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 188us/sample - loss: 0.7438 - accuracy: 0.5779 - f1_m: 0.5747 - precision_m: 0.5747 - recall_m: 0.5747\n",
      "Epoch 32/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.8000 - accuracy: 0.5483 - f1_m: 0.5483 - precision_m: 0.5483 - recall_m: 0.5483WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 189us/sample - loss: 0.7568 - accuracy: 0.5707 - f1_m: 0.5677 - precision_m: 0.5677 - recall_m: 0.5677\n",
      "Epoch 33/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.7948 - accuracy: 0.5250 - f1_m: 0.5250 - precision_m: 0.5250 - recall_m: 0.5250WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 189us/sample - loss: 0.7597 - accuracy: 0.5471 - f1_m: 0.5608 - precision_m: 0.5608 - recall_m: 0.5608\n",
      "Epoch 34/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.7389 - accuracy: 0.5437 - f1_m: 0.5437 - precision_m: 0.5437 - recall_m: 0.5437WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 193us/sample - loss: 0.7208 - accuracy: 0.5634 - f1_m: 0.5608 - precision_m: 0.5608 - recall_m: 0.5608\n",
      "Epoch 35/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6823 - accuracy: 0.5824 - f1_m: 0.5824 - precision_m: 0.5824 - recall_m: 0.5824WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 188us/sample - loss: 0.6962 - accuracy: 0.5924 - f1_m: 0.5937 - precision_m: 0.5938 - recall_m: 0.5938\n",
      "Epoch 36/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6982 - accuracy: 0.6193 - f1_m: 0.6193 - precision_m: 0.6193 - recall_m: 0.6193WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 188us/sample - loss: 0.6937 - accuracy: 0.6033 - f1_m: 0.6042 - precision_m: 0.6042 - recall_m: 0.6042\n",
      "Epoch 37/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.7169 - accuracy: 0.6187 - f1_m: 0.6187 - precision_m: 0.6187 - recall_m: 0.6187WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 190us/sample - loss: 0.7494 - accuracy: 0.5978 - f1_m: 0.5937 - precision_m: 0.5938 - recall_m: 0.5938\n",
      "Epoch 38/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6961 - accuracy: 0.6094 - f1_m: 0.6094 - precision_m: 0.6094 - recall_m: 0.6094WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 192us/sample - loss: 0.7100 - accuracy: 0.5996 - f1_m: 0.5955 - precision_m: 0.5955 - recall_m: 0.5955\n",
      "Epoch 39/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.7238 - accuracy: 0.5375 - f1_m: 0.5375 - precision_m: 0.5375 - recall_m: 0.5375WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 186us/sample - loss: 0.6890 - accuracy: 0.5797 - f1_m: 0.5764 - precision_m: 0.5764 - recall_m: 0.5764\n",
      "Epoch 40/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6829 - accuracy: 0.5966 - f1_m: 0.5966 - precision_m: 0.5966 - recall_m: 0.5966WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 179us/sample - loss: 0.6737 - accuracy: 0.6123 - f1_m: 0.6233 - precision_m: 0.6233 - recall_m: 0.6233\n",
      "Epoch 41/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6577 - accuracy: 0.5909 - f1_m: 0.5909 - precision_m: 0.5909 - recall_m: 0.5909WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 177us/sample - loss: 0.6763 - accuracy: 0.5851 - f1_m: 0.5764 - precision_m: 0.5764 - recall_m: 0.5764\n",
      "Epoch 42/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6738 - accuracy: 0.6108 - f1_m: 0.6108 - precision_m: 0.6108 - recall_m: 0.6108WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 183us/sample - loss: 0.6761 - accuracy: 0.5888 - f1_m: 0.5903 - precision_m: 0.5903 - recall_m: 0.5903\n",
      "Epoch 43/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6753 - accuracy: 0.5994 - f1_m: 0.5994 - precision_m: 0.5994 - recall_m: 0.5994WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 179us/sample - loss: 0.6781 - accuracy: 0.5888 - f1_m: 0.5903 - precision_m: 0.5903 - recall_m: 0.5903\n",
      "Epoch 44/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.7007 - accuracy: 0.6031 - f1_m: 0.6031 - precision_m: 0.6031 - recall_m: 0.6031WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 185us/sample - loss: 0.6843 - accuracy: 0.5960 - f1_m: 0.5972 - precision_m: 0.5972 - recall_m: 0.5972\n",
      "Epoch 45/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6617 - accuracy: 0.6023 - f1_m: 0.6023 - precision_m: 0.6023 - recall_m: 0.6023WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 180us/sample - loss: 0.6483 - accuracy: 0.6087 - f1_m: 0.6094 - precision_m: 0.6094 - recall_m: 0.6094\n",
      "Epoch 46/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6883 - accuracy: 0.6619 - f1_m: 0.6619 - precision_m: 0.6619 - recall_m: 0.6619WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 181us/sample - loss: 0.6845 - accuracy: 0.6486 - f1_m: 0.6476 - precision_m: 0.6476 - recall_m: 0.6476\n",
      "Epoch 47/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6722 - accuracy: 0.6051 - f1_m: 0.6051 - precision_m: 0.6051 - recall_m: 0.6051WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 184us/sample - loss: 0.6780 - accuracy: 0.6123 - f1_m: 0.6076 - precision_m: 0.6076 - recall_m: 0.6076\n",
      "Epoch 48/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6367 - accuracy: 0.6420 - f1_m: 0.6420 - precision_m: 0.6420 - recall_m: 0.6420WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 179us/sample - loss: 0.6492 - accuracy: 0.6123 - f1_m: 0.6128 - precision_m: 0.6128 - recall_m: 0.6128\n",
      "Epoch 49/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6924 - accuracy: 0.5813 - f1_m: 0.5812 - precision_m: 0.5813 - recall_m: 0.5813WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 183us/sample - loss: 0.6730 - accuracy: 0.5924 - f1_m: 0.5990 - precision_m: 0.5990 - recall_m: 0.5990\n",
      "Epoch 50/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6462 - accuracy: 0.6420 - f1_m: 0.6420 - precision_m: 0.6420 - recall_m: 0.6420WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 182us/sample - loss: 0.6595 - accuracy: 0.6105 - f1_m: 0.6111 - precision_m: 0.6111 - recall_m: 0.6111\n",
      "Epoch 51/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6802 - accuracy: 0.6562 - f1_m: 0.6562 - precision_m: 0.6562 - recall_m: 0.6562WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 183us/sample - loss: 0.6765 - accuracy: 0.6413 - f1_m: 0.6354 - precision_m: 0.6354 - recall_m: 0.6354\n",
      "Epoch 52/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6880 - accuracy: 0.6136 - f1_m: 0.6136 - precision_m: 0.6136 - recall_m: 0.6136WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 183us/sample - loss: 0.7084 - accuracy: 0.5978 - f1_m: 0.5990 - precision_m: 0.5990 - recall_m: 0.5990\n",
      "Epoch 53/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6325 - accuracy: 0.6080 - f1_m: 0.6080 - precision_m: 0.6080 - recall_m: 0.6080WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 182us/sample - loss: 0.6497 - accuracy: 0.6014 - f1_m: 0.6076 - precision_m: 0.6076 - recall_m: 0.6076\n",
      "Epoch 54/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6760 - accuracy: 0.5969 - f1_m: 0.5969 - precision_m: 0.5969 - recall_m: 0.5969WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 194us/sample - loss: 0.6625 - accuracy: 0.5996 - f1_m: 0.6059 - precision_m: 0.6059 - recall_m: 0.6059\n",
      "Epoch 55/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6373 - accuracy: 0.6136 - f1_m: 0.6136 - precision_m: 0.6136 - recall_m: 0.6136WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 187us/sample - loss: 0.6425 - accuracy: 0.6105 - f1_m: 0.6111 - precision_m: 0.6111 - recall_m: 0.6111\n",
      "Epoch 56/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6464 - accuracy: 0.6392 - f1_m: 0.6392 - precision_m: 0.6392 - recall_m: 0.6392WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 185us/sample - loss: 0.6408 - accuracy: 0.6232 - f1_m: 0.6337 - precision_m: 0.6337 - recall_m: 0.6337\n",
      "Epoch 57/100\n",
      "352/552 [==================>...........] - ETA: 0s - loss: 0.6651 - accuracy: 0.6193 - f1_m: 0.6193 - precision_m: 0.6193 - recall_m: 0.6193WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 189us/sample - loss: 0.6567 - accuracy: 0.6322 - f1_m: 0.6319 - precision_m: 0.6319 - recall_m: 0.6319\n",
      "Epoch 58/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6812 - accuracy: 0.6125 - f1_m: 0.6125 - precision_m: 0.6125 - recall_m: 0.6125WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 194us/sample - loss: 0.6547 - accuracy: 0.6123 - f1_m: 0.6076 - precision_m: 0.6076 - recall_m: 0.6076\n",
      "Epoch 59/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6116 - accuracy: 0.6438 - f1_m: 0.6437 - precision_m: 0.6438 - recall_m: 0.6438WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 193us/sample - loss: 0.6295 - accuracy: 0.6359 - f1_m: 0.6406 - precision_m: 0.6406 - recall_m: 0.6406\n",
      "Epoch 60/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6238 - accuracy: 0.6469 - f1_m: 0.6469 - precision_m: 0.6469 - recall_m: 0.6469WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 192us/sample - loss: 0.6458 - accuracy: 0.6196 - f1_m: 0.6094 - precision_m: 0.6094 - recall_m: 0.6094\n",
      "Epoch 61/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6567 - accuracy: 0.6156 - f1_m: 0.6156 - precision_m: 0.6156 - recall_m: 0.6156WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 192us/sample - loss: 0.6477 - accuracy: 0.6250 - f1_m: 0.6198 - precision_m: 0.6198 - recall_m: 0.6198\n",
      "Epoch 62/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6231 - accuracy: 0.6250 - f1_m: 0.6250 - precision_m: 0.6250 - recall_m: 0.6250WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 197us/sample - loss: 0.6485 - accuracy: 0.5978 - f1_m: 0.5990 - precision_m: 0.5990 - recall_m: 0.5990\n",
      "Epoch 63/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6384 - accuracy: 0.6750 - f1_m: 0.6750 - precision_m: 0.6750 - recall_m: 0.6750WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 205us/sample - loss: 0.6387 - accuracy: 0.6721 - f1_m: 0.6858 - precision_m: 0.6858 - recall_m: 0.6858\n",
      "Epoch 64/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6364 - accuracy: 0.6406 - f1_m: 0.6406 - precision_m: 0.6406 - recall_m: 0.6406WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 196us/sample - loss: 0.6312 - accuracy: 0.6286 - f1_m: 0.6285 - precision_m: 0.6285 - recall_m: 0.6285\n",
      "Epoch 65/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6401 - accuracy: 0.6375 - f1_m: 0.6375 - precision_m: 0.6375 - recall_m: 0.6375WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 199us/sample - loss: 0.6461 - accuracy: 0.6286 - f1_m: 0.6285 - precision_m: 0.6285 - recall_m: 0.6285\n",
      "Epoch 66/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6268 - accuracy: 0.6438 - f1_m: 0.6437 - precision_m: 0.6438 - recall_m: 0.6438WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 201us/sample - loss: 0.6274 - accuracy: 0.6341 - f1_m: 0.6337 - precision_m: 0.6337 - recall_m: 0.6337\n",
      "Epoch 67/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6360 - accuracy: 0.6438 - f1_m: 0.6437 - precision_m: 0.6438 - recall_m: 0.6438WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 207us/sample - loss: 0.6420 - accuracy: 0.6467 - f1_m: 0.6354 - precision_m: 0.6354 - recall_m: 0.6354\n",
      "Epoch 68/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6447 - accuracy: 0.6281 - f1_m: 0.6281 - precision_m: 0.6281 - recall_m: 0.6281WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 204us/sample - loss: 0.6396 - accuracy: 0.6214 - f1_m: 0.6163 - precision_m: 0.6163 - recall_m: 0.6163\n",
      "Epoch 69/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6095 - accuracy: 0.6625 - f1_m: 0.6625 - precision_m: 0.6625 - recall_m: 0.6625WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 207us/sample - loss: 0.6355 - accuracy: 0.6395 - f1_m: 0.6389 - precision_m: 0.6389 - recall_m: 0.6389\n",
      "Epoch 70/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6465 - accuracy: 0.6469 - f1_m: 0.6469 - precision_m: 0.6469 - recall_m: 0.6469WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 209us/sample - loss: 0.6353 - accuracy: 0.6612 - f1_m: 0.6545 - precision_m: 0.6545 - recall_m: 0.6545\n",
      "Epoch 71/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6089 - accuracy: 0.6625 - f1_m: 0.6625 - precision_m: 0.6625 - recall_m: 0.6625WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 209us/sample - loss: 0.6220 - accuracy: 0.6304 - f1_m: 0.6302 - precision_m: 0.6302 - recall_m: 0.6302\n",
      "Epoch 72/100\n",
      "288/552 [==============>...............] - ETA: 0s - loss: 0.6843 - accuracy: 0.6007 - f1_m: 0.6007 - precision_m: 0.6007 - recall_m: 0.6007WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 218us/sample - loss: 0.6522 - accuracy: 0.6341 - f1_m: 0.6389 - precision_m: 0.6389 - recall_m: 0.6389\n",
      "Epoch 73/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6376 - accuracy: 0.6344 - f1_m: 0.6344 - precision_m: 0.6344 - recall_m: 0.6344WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 209us/sample - loss: 0.6475 - accuracy: 0.6214 - f1_m: 0.6267 - precision_m: 0.6267 - recall_m: 0.6267\n",
      "Epoch 74/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6634 - accuracy: 0.5938 - f1_m: 0.5937 - precision_m: 0.5938 - recall_m: 0.5938WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 210us/sample - loss: 0.6586 - accuracy: 0.6087 - f1_m: 0.6146 - precision_m: 0.6146 - recall_m: 0.6146\n",
      "Epoch 75/100\n",
      "320/552 [================>.............] - ETA: 0s - loss: 0.6448 - accuracy: 0.6187 - f1_m: 0.6187 - precision_m: 0.6187 - recall_m: 0.6187WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 212us/sample - loss: 0.6354 - accuracy: 0.6377 - f1_m: 0.6424 - precision_m: 0.6424 - recall_m: 0.6424\n",
      "Epoch 76/100\n",
      "544/552 [============================>.] - ETA: 0s - loss: 0.6201 - accuracy: 0.6507 - f1_m: 0.6507 - precision_m: 0.6507 - recall_m: 0.6507WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 230us/sample - loss: 0.6173 - accuracy: 0.6540 - f1_m: 0.6632 - precision_m: 0.6632 - recall_m: 0.6632\n",
      "Epoch 77/100\n",
      "544/552 [============================>.] - ETA: 0s - loss: 0.6302 - accuracy: 0.6544 - f1_m: 0.6544 - precision_m: 0.6544 - recall_m: 0.6544WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 223us/sample - loss: 0.6292 - accuracy: 0.6558 - f1_m: 0.6597 - precision_m: 0.6597 - recall_m: 0.6597\n",
      "Epoch 78/100\n",
      "544/552 [============================>.] - ETA: 0s - loss: 0.6050 - accuracy: 0.6452 - f1_m: 0.6452 - precision_m: 0.6452 - recall_m: 0.6452WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 221us/sample - loss: 0.6049 - accuracy: 0.6467 - f1_m: 0.6510 - precision_m: 0.6510 - recall_m: 0.6510\n",
      "Epoch 79/100\n",
      "544/552 [============================>.] - ETA: 0s - loss: 0.6220 - accuracy: 0.6287 - f1_m: 0.6287 - precision_m: 0.6287 - recall_m: 0.6287WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 224us/sample - loss: 0.6191 - accuracy: 0.6304 - f1_m: 0.6354 - precision_m: 0.6354 - recall_m: 0.6354\n",
      "Epoch 80/100\n",
      "544/552 [============================>.] - ETA: 0s - loss: 0.6506 - accuracy: 0.6434 - f1_m: 0.6434 - precision_m: 0.6434 - recall_m: 0.6434WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 236us/sample - loss: 0.6503 - accuracy: 0.6413 - f1_m: 0.6354 - precision_m: 0.6354 - recall_m: 0.6354\n",
      "Epoch 81/100\n",
      "544/552 [============================>.] - ETA: 0s - loss: 0.6223 - accuracy: 0.6489 - f1_m: 0.6489 - precision_m: 0.6489 - recall_m: 0.6489WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 233us/sample - loss: 0.6228 - accuracy: 0.6467 - f1_m: 0.6406 - precision_m: 0.6406 - recall_m: 0.6406\n",
      "Epoch 82/100\n",
      "544/552 [============================>.] - ETA: 0s - loss: 0.6208 - accuracy: 0.6526 - f1_m: 0.6526 - precision_m: 0.6526 - recall_m: 0.6526WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 237us/sample - loss: 0.6283 - accuracy: 0.6486 - f1_m: 0.6372 - precision_m: 0.6372 - recall_m: 0.6372\n",
      "Epoch 83/100\n",
      "544/552 [============================>.] - ETA: 0s - loss: 0.6268 - accuracy: 0.6379 - f1_m: 0.6379 - precision_m: 0.6379 - recall_m: 0.6379WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 234us/sample - loss: 0.6245 - accuracy: 0.6413 - f1_m: 0.6510 - precision_m: 0.6510 - recall_m: 0.6510\n",
      "Epoch 84/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6320 - accuracy: 0.6458 - f1_m: 0.6458 - precision_m: 0.6458 - recall_m: 0.6458WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 259us/sample - loss: 0.6274 - accuracy: 0.6504 - f1_m: 0.6545 - precision_m: 0.6545 - recall_m: 0.6545\n",
      "Epoch 85/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6081 - accuracy: 0.6542 - f1_m: 0.6542 - precision_m: 0.6542 - recall_m: 0.6542WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 253us/sample - loss: 0.6292 - accuracy: 0.6486 - f1_m: 0.6580 - precision_m: 0.6580 - recall_m: 0.6580\n",
      "Epoch 86/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6472 - accuracy: 0.6062 - f1_m: 0.6062 - precision_m: 0.6062 - recall_m: 0.6062WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 254us/sample - loss: 0.6493 - accuracy: 0.6069 - f1_m: 0.6181 - precision_m: 0.6181 - recall_m: 0.6181\n",
      "Epoch 87/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6332 - accuracy: 0.6313 - f1_m: 0.6312 - precision_m: 0.6313 - recall_m: 0.6313WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 258us/sample - loss: 0.6319 - accuracy: 0.6304 - f1_m: 0.6302 - precision_m: 0.6302 - recall_m: 0.6302\n",
      "Epoch 88/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6277 - accuracy: 0.6292 - f1_m: 0.6292 - precision_m: 0.6292 - recall_m: 0.6292WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 253us/sample - loss: 0.6400 - accuracy: 0.6178 - f1_m: 0.6128 - precision_m: 0.6128 - recall_m: 0.6128\n",
      "Epoch 89/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6452 - accuracy: 0.6313 - f1_m: 0.6312 - precision_m: 0.6313 - recall_m: 0.6313WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 257us/sample - loss: 0.6421 - accuracy: 0.6359 - f1_m: 0.6406 - precision_m: 0.6406 - recall_m: 0.6406\n",
      "Epoch 90/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6342 - accuracy: 0.6458 - f1_m: 0.6458 - precision_m: 0.6458 - recall_m: 0.6458WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 258us/sample - loss: 0.6238 - accuracy: 0.6576 - f1_m: 0.6667 - precision_m: 0.6667 - recall_m: 0.6667\n",
      "Epoch 91/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6229 - accuracy: 0.6562 - f1_m: 0.6562 - precision_m: 0.6562 - recall_m: 0.6562WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 265us/sample - loss: 0.6205 - accuracy: 0.6467 - f1_m: 0.6406 - precision_m: 0.6406 - recall_m: 0.6406\n",
      "Epoch 92/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6135 - accuracy: 0.6479 - f1_m: 0.6479 - precision_m: 0.6479 - recall_m: 0.6479WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 263us/sample - loss: 0.6342 - accuracy: 0.6286 - f1_m: 0.6233 - precision_m: 0.6233 - recall_m: 0.6233\n",
      "Epoch 93/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6272 - accuracy: 0.6354 - f1_m: 0.6354 - precision_m: 0.6354 - recall_m: 0.6354WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 265us/sample - loss: 0.6258 - accuracy: 0.6322 - f1_m: 0.6215 - precision_m: 0.6215 - recall_m: 0.6215\n",
      "Epoch 94/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6085 - accuracy: 0.6687 - f1_m: 0.6687 - precision_m: 0.6687 - recall_m: 0.6687WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 268us/sample - loss: 0.6137 - accuracy: 0.6612 - f1_m: 0.6597 - precision_m: 0.6597 - recall_m: 0.6597\n",
      "Epoch 95/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6259 - accuracy: 0.6438 - f1_m: 0.6437 - precision_m: 0.6438 - recall_m: 0.6438WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 264us/sample - loss: 0.6272 - accuracy: 0.6522 - f1_m: 0.6510 - precision_m: 0.6510 - recall_m: 0.6510\n",
      "Epoch 96/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6026 - accuracy: 0.6604 - f1_m: 0.6604 - precision_m: 0.6604 - recall_m: 0.6604WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 270us/sample - loss: 0.6110 - accuracy: 0.6576 - f1_m: 0.6562 - precision_m: 0.6562 - recall_m: 0.6562\n",
      "Epoch 97/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6213 - accuracy: 0.6750 - f1_m: 0.6750 - precision_m: 0.6750 - recall_m: 0.6750WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 265us/sample - loss: 0.6305 - accuracy: 0.6685 - f1_m: 0.6615 - precision_m: 0.6615 - recall_m: 0.6615\n",
      "Epoch 98/100\n",
      "480/552 [=========================>....] - ETA: 0s - loss: 0.6505 - accuracy: 0.6292 - f1_m: 0.6292 - precision_m: 0.6292 - recall_m: 0.6292WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 272us/sample - loss: 0.6428 - accuracy: 0.6322 - f1_m: 0.6215 - precision_m: 0.6215 - recall_m: 0.6215\n",
      "Epoch 99/100\n",
      "448/552 [=======================>......] - ETA: 0s - loss: 0.6371 - accuracy: 0.6183 - f1_m: 0.6183 - precision_m: 0.6183 - recall_m: 0.6183WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 326us/sample - loss: 0.6315 - accuracy: 0.6286 - f1_m: 0.6233 - precision_m: 0.6233 - recall_m: 0.6233\n",
      "Epoch 100/100\n",
      "416/552 [=====================>........] - ETA: 0s - loss: 0.6010 - accuracy: 0.6466 - f1_m: 0.6466 - precision_m: 0.6466 - recall_m: 0.6466WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "552/552 [==============================] - 0s 314us/sample - loss: 0.6031 - accuracy: 0.6395 - f1_m: 0.6441 - precision_m: 0.6441 - recall_m: 0.6441\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fe00cea9690>"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"../models/audio_model.h5\", verbose=True, save_best_only=True)\n",
    "\n",
    "model.fit(X_audio_train, y_audio_train, batch_size=num_batch_size, epochs=num_epochs, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa0dfd",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Audio Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2355a877",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.5907537319521973\n",
      "accuracy:0.6594203114509583\n",
      "f1_score:0.6787499189376831\n",
      "precision:0.6787499785423279\n",
      "recall:0.6787499785423279\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X_audio_test, y_audio_test, verbose=0)\n",
    "print(f\"loss: {loss}\\naccuracy:{accuracy}\\nf1_score:{f1_score}\\nprecision:{precision}\\nrecall:{recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "31bd5b74",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_file_path = \"mmsd_raw_data/converted_utterances/2_626.wav\"\n",
    "audio, sample_rate = librosa.load(test_file_path)\n",
    "mfcc_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "mfcc_scaled_features = np.mean(mfcc_features.T, axis=0)\n",
    "\n",
    "mfcc_scaled_features = mfcc_scaled_features.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "83c2dea3",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic: 1\n",
      "[[2.7844343e-21 1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "predicted_label = model.predict_classes(mfcc_scaled_features)\n",
    "predicted_class = label_encoder.inverse_transform(predicted_label)\n",
    "print(f\"Sarcastic: {predicted_class[0]}\")\n",
    "\n",
    "x = model.predict_proba(mfcc_scaled_features)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75257a76",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aggregator Model"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 15:05:41.285896: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-12 15:05:41.287490: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "models_dir = \"models\"\n",
    "\n",
    "text_model = load_model(f\"{models_dir}/text_model.h5\")\n",
    "audio_model = load_model(f\"{models_dir}/audio_model.h5\")\n",
    "\n",
    "file_lookup_index = 686\n",
    "text_model_weight = .195\n",
    "audio_model_weight = 1 - text_model_weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "outputs": [],
   "source": [
    "testd_token_statement = tokenizer.texts_to_sequences([data['utterance'][file_lookup_index]])\n",
    "testd_token_statement = pad_sequences(testd_token_statement, maxlen=max_length, padding=padding_type)\n",
    "testd_output = text_model.predict(tokenized_statement)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "outputs": [],
   "source": [
    "testd_file_path = f\"mmsd_raw_data/converted_utterances/{data['file_name'][file_lookup_index]}\"\n",
    "testd_audio, testd_sample_rate = librosa.load(testd_file_path)\n",
    "testd_mfcc_features = librosa.feature.mfcc(y=testd_audio, sr=testd_sample_rate, n_mfcc=13)\n",
    "testd_mfcc_scaled_features = np.mean(testd_mfcc_features.T, axis=0)\n",
    "testd_mfcc_scaled_features = testd_mfcc_scaled_features.reshape(1, -1)\n",
    "testd_predicted_label = audio_model.predict(testd_mfcc_scaled_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: 2_235.wav\n",
      "Utterance: Oh yeah he has a caretaker his older brother, Ernie. You can't make this stuff up!\n",
      "Sarcastic: 0\n",
      "Text Prediction [[0.92711174 0.07288828]]\n",
      "Audio Prediction [[0.24476345 0.7552366 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"File Name:\",data['file_name'][file_lookup_index])\n",
    "print(\"Utterance:\",data['utterance'][file_lookup_index])\n",
    "print(\"Sarcastic:\",data['sarcasm'][file_lookup_index])\n",
    "print(\"Text Prediction\", testd_output)\n",
    "print(\"Audio Prediction\", testd_predicted_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "outputs": [
    {
     "data": {
      "text/plain": "0.37782136656343934"
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_0 = (testd_predicted_label[0][0]*audio_model_weight + testd_output[0][0]*text_model_weight)\n",
    "total_0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "outputs": [
    {
     "data": {
      "text/plain": "0.622178697772324"
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_1 = (testd_predicted_label[0][1]*audio_model_weight + testd_output[0][1]*text_model_weight)\n",
    "total_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic\n"
     ]
    }
   ],
   "source": [
    "total = total_0 - total_1\n",
    "\n",
    "if total <= 0:\n",
    "    print(\"Sarcastic\")\n",
    "else:\n",
    "    print(\"Non Sarcastic\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metric Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score,f1_score, ConfusionMatrixDisplay\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime as dt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "690it [06:14,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "test_y_actual = []\n",
    "test_y_pred = []\n",
    "\n",
    "prediction_timings = []\n",
    "\n",
    "for index,row in tqdm(data.iterrows()):\n",
    "    start = dt.now()\n",
    "    test_statement = tokenizer.texts_to_sequences(row['utterance'])\n",
    "    test_statement = pad_sequences(test_statement, maxlen=max_length, padding=padding_type)\n",
    "\n",
    "    testd_file_path = f\"mmsd_raw_data/converted_utterances/{row['file_name']}\"\n",
    "    test_audio, test_audio_sr = librosa.load(testd_file_path)\n",
    "    test_audio_features = librosa.feature.mfcc(y=test_audio, sr=test_audio_sr, n_mfcc=13)\n",
    "    test_mfcc_scaled_features = np.mean(test_audio_features.T, axis=0)\n",
    "    test_mfcc_scaled_features = test_mfcc_scaled_features.reshape(1, -1)\n",
    "\n",
    "    actual_y = 1 if row['sarcasm'] == True else 0\n",
    "\n",
    "    text_prediction = text_model.predict(test_statement)\n",
    "    audio_prediction = audio_model.predict(test_mfcc_scaled_features)\n",
    "\n",
    "    combined_1 = (audio_prediction[0][0]*audio_model_weight + text_prediction[0][0]*text_model_weight)\n",
    "    combined_2 = (audio_prediction[0][1]*audio_model_weight + text_prediction[0][1]*text_model_weight)\n",
    "\n",
    "    running_time = (dt.now() - start).seconds\n",
    "    prediction_timings.append(running_time)\n",
    "    test_y_actual.append(actual_y)\n",
    "    test_y_pred.append(1 if combined_1 - combined_2 <= 0 else 0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix without normalizing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_true=test_y_actual, y_pred=test_y_pred, cmap=\"Blues\", display_labels=[\"False\", \"True\"], colorbar=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix with Normalizing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfm = ConfusionMatrixDisplay.from_predictions(y_true=test_y_actual, y_pred=test_y_pred, cmap=\"Blues\", display_labels=[\"False\", \"True\"], colorbar=False, normalize=\"all\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "rs = recall_score(test_y_actual, test_y_pred)\n",
    "ps = precision_score(test_y_actual, test_y_pred)\n",
    "acc_score = accuracy_score(test_y_actual, test_y_pred)\n",
    "f1_score = f1_score(test_y_actual, test_y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.4\t\t\tPrecision: 0.83\n",
      "Accuracy: 0.66\t\tF1 Score: 0.54\n"
     ]
    }
   ],
   "source": [
    "print(f\"Recall: {round(rs, 2)}\\t\\t\\tPrecision: {round(ps, 2)}\\nAccuracy: {round(acc_score, 2)}\\t\\tF1 Score: {round(f1_score, 2)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}